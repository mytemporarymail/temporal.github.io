import os
import time
import json
import subprocess
import requests
import logging
import yaml
import hashlib
import shutil

from bs4 import BeautifulSoup
from markdownify import markdownify as md

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager


# ========================
# æ—¥å¿—é…ç½®
# ========================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    datefmt="%H:%M:%S"
)

logger = logging.getLogger(__name__)


# ========================
# é…ç½®åŒº
# ========================

OUTPUT_DIR = "markdown"  # æºæ–‡ä»¶ç›®å½•
IMAGE_DIR = os.path.join(OUTPUT_DIR, "images")
CONFIG_FILE = "articles.txt"
DOWNLOADED_FILE = "downloaded.json"  # è®°å½•å·²ä¸‹è½½çš„URL


def load_downloaded_urls():
    """åŠ è½½å·²ä¸‹è½½çš„URLåˆ—è¡¨ï¼ˆè¿”å›å­—å…¸ï¼šURL -> æ–‡ä»¶ä¿¡æ¯ï¼‰"""
    if not os.path.exists(DOWNLOADED_FILE):
        return {}
    
    try:
        with open(DOWNLOADED_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            # å…¼å®¹æ—§æ ¼å¼ï¼ˆä»… URL åˆ—è¡¨ï¼‰
            if isinstance(data.get("urls"), list):
                return {url: {"filename": None} for url in data.get("urls", [])}
            return data.get("articles", {})
    except Exception as e:
        logger.warning(f"è¯»å–å·²ä¸‹è½½è®°å½•å¤±è´¥: {e}")
        return {}


def save_downloaded_url(url, filename):
    """ä¿å­˜å·²ä¸‹è½½çš„URLåŠå…¶æ–‡ä»¶ä¿¡æ¯"""
    articles = load_downloaded_urls()
    articles[url] = {
        "filename": filename,
        "downloaded_at": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    try:
        with open(DOWNLOADED_FILE, "w", encoding="utf-8") as f:
            json.dump({"articles": articles}, f, indent=2, ensure_ascii=False)
    except Exception as e:
        logger.error(f"ä¿å­˜ä¸‹è½½è®°å½•å¤±è´¥: {e}")


def load_delete_urls():
    """åŠ è½½è¦åˆ é™¤çš„ URL åˆ—è¡¨"""
    if not os.path.exists(CONFIG_FILE):
        return []
    
    try:
        delete_urls = []
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line.startswith("#DELETE:"):
                    url = line.replace("#DELETE:", "").strip()
                    if url:
                        delete_urls.append(url)
        
        if delete_urls:
            logger.info(f"å‘ç° {len(delete_urls)} ç¯‡è¦åˆ é™¤çš„æ–‡ç« ")
        return delete_urls
    except Exception as e:
        logger.error(f"è¯»å–åˆ é™¤åˆ—è¡¨å¤±è´¥: {e}")
        return []


def delete_article(url):
    """åˆ é™¤æŒ‡å®š URL å¯¹åº”çš„æ–‡ç« æ–‡ä»¶å’Œå›¾ç‰‡"""
    articles = load_downloaded_urls()
    
    if url not in articles:
        logger.warning(f"æœªæ‰¾åˆ°è¯¥ URL çš„ä¸‹è½½è®°å½•: {url}")
        return False
    
    filename = articles[url].get("filename")
    logger.info(f"å¼€å§‹åˆ é™¤æ–‡ç« ï¼Œè®°å½•çš„æ–‡ä»¶å: {filename}")
    
    # å¦‚æœæ²¡æœ‰è®°å½•æ–‡ä»¶åï¼Œå°è¯•ä» markdown ç›®å½•ä¸­æŸ¥æ‰¾
    if not filename:
        logger.info("ğŸ“ æœªè®°å½•æ–‡ä»¶åï¼Œå°è¯•è‡ªåŠ¨æŸ¥æ‰¾...")
        if os.path.exists(OUTPUT_DIR):
            for f in os.listdir(OUTPUT_DIR):
                if f.endswith('.md'):
                    filepath = os.path.join(OUTPUT_DIR, f)
                    try:
                        with open(filepath, 'r', encoding='utf-8') as file:
                            content = file.read()
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«è¯¥ URLï¼ˆé€šå¸¸åœ¨æ–‡ç« ä¸­ä¼šå‡ºç°ï¼‰
                            if url in content:
                                filename = f
                                logger.info(f"âœ… æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {f}")
                                break
                    except:
                        pass
    
    # åˆ é™¤ markdown æ–‡ä»¶
    deleted = False
    if filename:
        filepath = os.path.join(OUTPUT_DIR, filename)
        logger.info(f"å‡†å¤‡åˆ é™¤: {filepath}")
        if os.path.exists(filepath):
            try:
                os.remove(filepath)
                logger.info(f"âœ… å·²åˆ é™¤æ–‡ç« æ–‡ä»¶: {filepath}")
                deleted = True
            except Exception as e:
                logger.error(f"âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥: {filepath} | {e}")
        else:
            logger.warning(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    else:
        logger.warning(f"âš ï¸  æœªèƒ½ç¡®å®šæ–‡ä»¶åï¼Œè·³è¿‡æ–‡ç« æ–‡ä»¶åˆ é™¤")
    
    # åˆ é™¤å¯¹åº”çš„å›¾ç‰‡ç›®å½•
    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
    logger.info(f"æŸ¥æ‰¾å¯¹åº”çš„å›¾ç‰‡ç›®å½•ï¼ˆURL hash: {url_hash}ï¼‰...")
    
    if os.path.exists(IMAGE_DIR):
        found_images = False
        for img_dir in os.listdir(IMAGE_DIR):
            if img_dir.endswith(f"_{url_hash}"):
                img_path = os.path.join(IMAGE_DIR, img_dir)
                try:
                    shutil.rmtree(img_path)
                    logger.info(f"âœ… å·²åˆ é™¤å›¾ç‰‡ç›®å½•: {img_path}")
                    found_images = True
                    deleted = True
                except Exception as e:
                    logger.error(f"âŒ åˆ é™¤å›¾ç‰‡ç›®å½•å¤±è´¥: {img_path} | {e}")
        
        if not found_images:
            logger.info(f"âš ï¸  æœªæ‰¾åˆ°å¯¹åº”çš„å›¾ç‰‡ç›®å½•")
    
    # ä»ä¸‹è½½è®°å½•ä¸­ç§»é™¤
    del articles[url]
    try:
        with open(DOWNLOADED_FILE, "w", encoding="utf-8") as f:
            json.dump({"articles": articles}, f, indent=2, ensure_ascii=False)
        logger.info(f"âœ… å·²ä»ä¸‹è½½è®°å½•ä¸­ç§»é™¤: {url}")
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°ä¸‹è½½è®°å½•å¤±è´¥: {e}")
    
    return deleted


def load_articles():
    """ä»é…ç½®æ–‡ä»¶åŠ è½½æ–‡ç« URLåˆ—è¡¨ï¼ˆæ’é™¤åˆ é™¤æ ‡è®°çš„ï¼‰"""
    if not os.path.exists(CONFIG_FILE):
        logger.error(f"é…ç½®æ–‡ä»¶ {CONFIG_FILE} ä¸å­˜åœ¨")
        return []
    
    try:
        urls = []
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œã€æ³¨é‡Šå’Œåˆ é™¤æ ‡è®°
                if line and not line.startswith("#"):
                    urls.append(line)
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(urls)} ä¸ªæ–‡ç« URL")
        return urls
    except Exception as e:
        logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å‡ºé”™: {e}")
        return []


# ========================


def init_driver():
    logger.info("åˆå§‹åŒ– Chrome æµè§ˆå™¨...")
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--lang=zh-CN")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    )

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)

    logger.info("ChromeDriver åˆå§‹åŒ–å®Œæˆ")
    return driver


def fetch_article_html(driver, url):
    logger.info(f"æ‰“å¼€æ–‡ç« é¡µé¢: {url}")
    driver.get(url)
    time.sleep(4)

    soup = BeautifulSoup(driver.page_source, "html.parser")

    # æå–æ–‡ç« æ ‡é¢˜
    title = None
    title_elem = soup.find("h1", id="js_title")
    if title_elem:
        title = title_elem.get_text(strip=True)
    
    if not title:
        title_tag = soup.find("title")
        if title_tag:
            title = title_tag.get_text(strip=True).replace(" - å¾®ä¿¡å…¬ä¼—å¹³å°", "").strip()
    
    if not title:
        title = "æœªçŸ¥æ ‡é¢˜"
    
    logger.info(f"æ–‡ç« æ ‡é¢˜: {title}")

    content = soup.find("div", id="js_content")
    if content is None:
        logger.warning("æœªæ‰¾åˆ° js_contentï¼Œå°è¯• rich_media_content")
        content = soup.find("div", class_="rich_media_content")

    if content is None:
        logger.error("æ–‡ç« å†…å®¹èŠ‚ç‚¹æœªæ‰¾åˆ°")
        raise Exception("æ— æ³•æ‰¾åˆ°æ–‡ç« å†…å®¹")

    logger.info("æˆåŠŸè§£ææ–‡ç« ä¸»ä½“å†…å®¹")
    return title, content


def download_image(url, filename):
    logger.info(f"ä¸‹è½½å›¾ç‰‡: {url}")
    r = requests.get(url, timeout=15)
    with open(filename, "wb") as f:
        f.write(r.content)


def process_images(html, title, url):
    logger.info("å¤„ç†æ–‡ç« ä¸­çš„å›¾ç‰‡...")
    
    # ç”¨ URL çš„å“ˆå¸Œå€¼æ¥åŒºåˆ†æ–‡ç« ï¼Œç¡®ä¿å”¯ä¸€æ€§ï¼ˆå³ä½¿æ ‡é¢˜é‡å¤ï¼‰
    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
    safe_title = "".join(c if c.isalnum() or c in "._- " else "" for c in title).strip()
    safe_title = safe_title.replace(" ", "_") or "article"
    
    # æ–‡ä»¶å¤¹åæ ¼å¼ï¼šæ ‡é¢˜_å“ˆå¸Œå€¼ï¼ˆå¯è¯»æ€§ + å”¯ä¸€æ€§ï¼‰
    article_image_dir = os.path.join(IMAGE_DIR, f"{safe_title}_{url_hash}")
    os.makedirs(article_image_dir, exist_ok=True)

    soup = BeautifulSoup(str(html), "html.parser")
    imgs = soup.find_all("img")

    logger.info(f"å‘ç° {len(imgs)} å¼ å›¾ç‰‡")

    for i, img in enumerate(imgs):
        src = img.get("data-src") or img.get("src")
        if not src:
            continue

        ext = ".jpg"
        if "png" in src:
            ext = ".png"

        filename = f"img_{i}{ext}"
        filepath = os.path.join(article_image_dir, filename)

        try:
            download_image(src, filepath)
            img["src"] = f"images/{safe_title}_{url_hash}/{filename}"
            logger.info(f"å›¾ç‰‡ä¿å­˜æˆåŠŸ: {filename}")
        except Exception as e:
            logger.error(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {src} | {e}")

    return str(soup)


def get_next_article_number():
    """è·å–ä¸‹ä¸€ä¸ªæ–‡ç« ç¼–å·"""
    markdown_dir = OUTPUT_DIR
    if not os.path.exists(markdown_dir):
        return 1
    
    # æŸ¥æ‰¾ç°æœ‰çš„æ•°å­—æ–‡ä»¶ï¼ˆ00X.md æ ¼å¼ï¼‰
    files = os.listdir(markdown_dir)
    max_num = 0
    for f in files:
        if f[0].isdigit() and f.endswith('.md'):
            try:
                num = int(f.split('.')[0])
                max_num = max(max_num, num)
            except:
                pass
    
    return max_num + 1


def save_markdown(title, html):
    logger.info("è½¬æ¢ä¸º Markdown æ ¼å¼...")
    
    # ç®€å•è½¬æ¢ä¸º markdown
    md_text = md(html)
    
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼ˆä½†ä¿ç•™æ®µè½é—´è·ï¼‰
    lines = md_text.split('\n')
    cleaned_lines = []
    prev_empty = False
    
    for line in lines:
        if line.strip():  # éç©ºè¡Œ
            cleaned_lines.append(line)
            prev_empty = False
        elif not prev_empty:  # åªä¿ç•™ä¸€ä¸ªç©ºè¡Œ
            cleaned_lines.append('')
            prev_empty = True
    
    # ç§»é™¤æœ«å°¾å¤šä½™ç©ºè¡Œ
    while cleaned_lines and not cleaned_lines[-1].strip():
        cleaned_lines.pop()
    
    md_text = '\n'.join(cleaned_lines)
    md_text = f"# {title}\n\n" + md_text

    # ä½¿ç”¨é€’å¢çš„æ•°å­—ä½œä¸ºæ–‡ä»¶åï¼ˆæ›´ç®€æ´çš„ URLï¼‰
    article_num = get_next_article_number()
    filename = f"{article_num:03d}.md"  # 001.md, 002.md ç­‰
    filepath = os.path.join(OUTPUT_DIR, filename)

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(md_text)

    logger.info(f"Markdown æ–‡ä»¶å·²ç”Ÿæˆ: {filepath}")
    return filename, title  # è¿”å›æ–‡ä»¶åå’ŒåŸå§‹æ ‡é¢˜


def update_mkdocs_nav(articles_files):
    """æ›´æ–° mkdocs.yml å¯¼èˆªï¼Œæ·»åŠ æ–°æ–‡ç« 
    articles_files: [(filename, title), ...] å…ƒç»„åˆ—è¡¨
    """
    mkdocs_file = "mkdocs.yml"
    
    try:
        with open(mkdocs_file, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f) or {}
        
        # åˆå§‹åŒ– nav
        if "nav" not in config:
            config["nav"] = []
        
        # è·å–ç°æœ‰çš„å¯¼èˆªé¡¹
        existing_files = set()
        new_nav = []
        
        for item in config["nav"]:
            if isinstance(item, dict):
                for title, path in item.items():
                    existing_files.add(path)
                    new_nav.append(item)
            else:
                new_nav.append(item)
        
        # æ·»åŠ æ–°æ–‡ç« åˆ°å¯¼èˆªï¼ˆä½¿ç”¨åŸå§‹æ ‡é¢˜å’Œæ•°å­—æ–‡ä»¶åï¼‰
        for filename, title in articles_files:
            if filename not in existing_files:
                new_nav.append({title: filename})
                logger.info(f"å°† '{title}' æ·»åŠ åˆ°å¯¼èˆª ({filename})")
        
        config["nav"] = new_nav
        
        # ä¿å­˜æ›´æ–°åçš„é…ç½®
        with open(mkdocs_file, "w", encoding="utf-8") as f:
            yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
        
        logger.info("mkdocs.yml å·²æ›´æ–°")
    except Exception as e:
        logger.error(f"æ›´æ–° mkdocs.yml å¤±è´¥: {e}")


def build_mkdocs():
    """è¿è¡Œ mkdocs build é‡æ–°æ„å»ºç½‘ç«™"""
    try:
        logger.info("å¼€å§‹æ„å»º MkDocs ç½‘ç«™...")
        # åœ¨å½“å‰å·¥ä½œç›®å½•è¿è¡Œï¼Œç¡®ä¿æ‰¾åˆ° mkdocs.yml
        result = subprocess.run(
            ["python", "-m", "mkdocs", "build"],
            cwd=os.getcwd(),
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            logger.info("MkDocs ç½‘ç«™æ„å»ºæˆåŠŸ âœ…")
        else:
            logger.error(f"MkDocs æ„å»ºå¤±è´¥: {result.stderr}")
    except Exception as e:
        logger.error(f"è¿è¡Œ mkdocs build å‡ºé”™: {e}")


def main():
    logger.info("===== ç¨‹åºå¯åŠ¨ =====")
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    has_changes = False  # æ ‡è®°æ˜¯å¦æœ‰æ–‡ç« æ”¹åŠ¨
    article_files_to_update = []  # æ–°æ·»åŠ çš„æ–‡ç« 

    # ============ ç¬¬ä¸€æ­¥ï¼šå¤„ç†åˆ é™¤æ“ä½œ ============
    logger.info("\n--- ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤æ–‡ç«  ---")
    delete_urls = load_delete_urls()
    if delete_urls:
        logger.info(f"å‡†å¤‡åˆ é™¤ {len(delete_urls)} ç¯‡æ–‡ç« ...")
        deleted_files = []
        for url in delete_urls:
            if delete_article(url):
                deleted_files.append(url)
                has_changes = True
                logger.info(f"âœ… æ–‡ç« å·²åˆ é™¤: {url}")
        
        # æ›´æ–° mkdocs.yml ç§»é™¤è¢«åˆ é™¤çš„æ–‡ç« 
        if deleted_files:
            mkdocs_file = "mkdocs.yml"
            try:
                with open(mkdocs_file, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f) or {}
                
                if "nav" in config:
                    # ä»å¯¼èˆªä¸­ç§»é™¤å·²åˆ é™¤çš„æ–‡ç« 
                    new_nav = [config["nav"][0]]  # ä¿ç•™é¦–é¡µ
                    
                    for item in config["nav"][1:]:  # è·³è¿‡é¦–é¡µ
                        if isinstance(item, dict):
                            for title, path in item.items():
                                # æ£€æŸ¥æ­¤æ¡ç›®æ˜¯å¦å¯¹åº”è¢«åˆ é™¤çš„æ–‡ç« 
                                is_deleted = False
                                for del_url in deleted_files:
                                    articles = load_downloaded_urls()
                                    if del_url in articles and articles[del_url].get("filename") == path:
                                        is_deleted = True
                                        break
                                
                                if not is_deleted:
                                    new_nav.append(item)
                        else:
                            new_nav.append(item)
                    
                    config["nav"] = new_nav
                    with open(mkdocs_file, "w", encoding="utf-8") as f:
                        yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
                    logger.info("âœ… mkdocs.yml å·²æ›´æ–°ï¼ˆç§»é™¤å·²åˆ é™¤çš„æ–‡ç« ï¼‰")
            except Exception as e:
                logger.error(f"âŒ æ›´æ–° mkdocs.yml å¤±è´¥: {e}")
    else:
        logger.info("æ— éœ€åˆ é™¤æ–‡ç« ")

    # ============ ç¬¬äºŒæ­¥ï¼šå¤„ç†ä¸‹è½½æ“ä½œ ============
    logger.info("\n--- ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ–°æ–‡ç« éœ€è¦ä¸‹è½½ ---")
    urls = load_articles()
    if not urls:
        logger.warning("âš ï¸  æ²¡æœ‰æ–‡ç«  URL éœ€è¦å¤„ç†")
    else:
        # åŠ è½½å·²ä¸‹è½½çš„URL
        downloaded_urls = load_downloaded_urls()
        logger.info(f"å·²ä¸‹è½½è®°å½•: {len(downloaded_urls)} ç¯‡æ–‡ç« ")
        
        # è¿‡æ»¤å‡ºæœªä¸‹è½½çš„URL
        urls_to_process = [url for url in urls if url not in downloaded_urls]
        
        if urls_to_process:
            logger.info(f"éœ€è¦å¤„ç†: {len(urls_to_process)} ç¯‡æ–°æ–‡ç« ")
            
            driver = init_driver()
            
            for url in urls_to_process:
                try:
                    title, html = fetch_article_html(driver, url)
                    logger.info(f"å¼€å§‹å¤„ç†æ–‡ç« : {title}")
                    html = process_images(html, title, url)
                    filename, actual_title = save_markdown(title, html)
                    article_files_to_update.append((filename, actual_title))
                    
                    # ä¿å­˜å·²ä¸‹è½½çš„URLå’Œæ–‡ä»¶å
                    save_downloaded_url(url, filename)
                    has_changes = True
                    logger.info(f"âœ… æ–‡ç« å®Œæˆ: {title}")
                except Exception as e:
                    logger.error(f"âŒ æ–‡ç« å¤„ç†å¤±è´¥: {url} | {e}")
            
            driver.quit()
            logger.info("æµè§ˆå™¨å·²å…³é—­")
            
            # æ›´æ–° mkdocs å¯¼èˆª
            if article_files_to_update:
                logger.info("æ­£åœ¨æ›´æ–° MkDocs é…ç½®...")
                update_mkdocs_nav(article_files_to_update)
        else:
            logger.info("æ‰€æœ‰æ–‡ç« éƒ½å·²ä¸‹è½½ï¼Œæ— æ–°æ–‡ç« å¤„ç†")

    # ============ ç¬¬ä¸‰æ­¥ï¼šé‡å»ºç½‘ç«™ ============
    logger.info("\n--- ç¬¬ä¸‰æ­¥ï¼šé‡æ–°æ„å»ºç½‘ç«™ ---")
    if has_changes:
        logger.info("æ£€æµ‹åˆ°æ–‡ç« æ”¹åŠ¨ï¼Œæ‰§è¡Œå®Œæ•´é‡å»º...")
        build_mkdocs()
    else:
        logger.info("âš ï¸  æ— æ–‡ç« æ”¹åŠ¨ï¼Œä½†æ‰§è¡Œä¸€æ¬¡ç½‘ç«™æ„å»º...")
        build_mkdocs()
    
    logger.info("\n===== å…¨éƒ¨ä»»åŠ¡å®Œæˆ âœ… =====")


if __name__ == "__main__":
    main()
