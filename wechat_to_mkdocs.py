import os
import time
import json
import subprocess
import requests
import logging
import yaml
import hashlib
import shutil

from bs4 import BeautifulSoup
from markdownify import markdownify as md

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager


# ========================
# æ—¥å¿—é…ç½®
# ========================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    datefmt="%H:%M:%S"
)

logger = logging.getLogger(__name__)


# ========================
# é…ç½®åŒº
# ========================

OUTPUT_DIR = "markdown"  # æºæ–‡ä»¶ç›®å½•
IMAGE_DIR = os.path.join(OUTPUT_DIR, "images")
CONFIG_FILE = "articles.txt"
DOWNLOADED_FILE = "downloaded.json"  # è®°å½•å·²ä¸‹è½½çš„URL


def load_downloaded_urls():
    """åŠ è½½å·²ä¸‹è½½çš„URLåˆ—è¡¨ï¼ˆè¿”å›å­—å…¸ï¼šURL -> æ–‡ä»¶ä¿¡æ¯ï¼‰"""
    if not os.path.exists(DOWNLOADED_FILE):
        return {}
    
    try:
        with open(DOWNLOADED_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            # å…¼å®¹æ—§æ ¼å¼ï¼ˆä»… URL åˆ—è¡¨ï¼‰
            if isinstance(data.get("urls"), list):
                return {url: {"filename": None} for url in data.get("urls", [])}
            return data.get("articles", {})
    except Exception as e:
        logger.warning(f"è¯»å–å·²ä¸‹è½½è®°å½•å¤±è´¥: {e}")
        return {}


def save_downloaded_url(url, filename):
    """ä¿å­˜å·²ä¸‹è½½çš„URLåŠå…¶æ–‡ä»¶ä¿¡æ¯"""
    articles = load_downloaded_urls()
    articles[url] = {
        "filename": filename,
        "downloaded_at": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    try:
        with open(DOWNLOADED_FILE, "w", encoding="utf-8") as f:
            json.dump({"articles": articles}, f, indent=2, ensure_ascii=False)
    except Exception as e:
        logger.error(f"ä¿å­˜ä¸‹è½½è®°å½•å¤±è´¥: {e}")


def load_delete_urls():
    """åŠ è½½è¦åˆ é™¤çš„ URL åˆ—è¡¨"""
    if not os.path.exists(CONFIG_FILE):
        return []
    
    try:
        delete_urls = []
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line.startswith("#DELETE:"):
                    url = line.replace("#DELETE:", "").strip()
                    if url:
                        delete_urls.append(url)
        
        if delete_urls:
            logger.info(f"å‘ç° {len(delete_urls)} ç¯‡è¦åˆ é™¤çš„æ–‡ç« ")
        return delete_urls
    except Exception as e:
        logger.error(f"è¯»å–åˆ é™¤åˆ—è¡¨å¤±è´¥: {e}")
        return []


def delete_article(url):
    """åˆ é™¤æŒ‡å®š URL å¯¹åº”çš„æ–‡ç« æ–‡ä»¶å’Œå›¾ç‰‡"""
    articles = load_downloaded_urls()
    
    if url not in articles:
        logger.warning(f"æœªæ‰¾åˆ°è¯¥ URL çš„ä¸‹è½½è®°å½•: {url}")
        return False
    
    filename = articles[url].get("filename")
    logger.info(f"å¼€å§‹åˆ é™¤æ–‡ç« ï¼Œè®°å½•çš„æ–‡ä»¶å: {filename}")
    
    # å¦‚æœæ²¡æœ‰è®°å½•æ–‡ä»¶åï¼Œå°è¯•ä» markdown ç›®å½•ä¸­æŸ¥æ‰¾
    if not filename:
        logger.info("ğŸ“ æœªè®°å½•æ–‡ä»¶åï¼Œå°è¯•è‡ªåŠ¨æŸ¥æ‰¾...")
        if os.path.exists(OUTPUT_DIR):
            for f in os.listdir(OUTPUT_DIR):
                if f.endswith('.md'):
                    filepath = os.path.join(OUTPUT_DIR, f)
                    try:
                        with open(filepath, 'r', encoding='utf-8') as file:
                            content = file.read()
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«è¯¥ URLï¼ˆé€šå¸¸åœ¨æ–‡ç« ä¸­ä¼šå‡ºç°ï¼‰
                            if url in content:
                                filename = f
                                logger.info(f"âœ… æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {f}")
                                break
                    except:
                        pass
    
    # åˆ é™¤ markdown æ–‡ä»¶
    deleted = False
    if filename:
        filepath = os.path.join(OUTPUT_DIR, filename)
        logger.info(f"å‡†å¤‡åˆ é™¤: {filepath}")
        if os.path.exists(filepath):
            try:
                os.remove(filepath)
                logger.info(f"âœ… å·²åˆ é™¤æ–‡ç« æ–‡ä»¶: {filepath}")
                deleted = True
            except Exception as e:
                logger.error(f"âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥: {filepath} | {e}")
        else:
            logger.warning(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    else:
        logger.warning(f"âš ï¸  æœªèƒ½ç¡®å®šæ–‡ä»¶åï¼Œè·³è¿‡æ–‡ç« æ–‡ä»¶åˆ é™¤")
    
    # åˆ é™¤å¯¹åº”çš„å›¾ç‰‡ç›®å½•
    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
    logger.info(f"æŸ¥æ‰¾å¯¹åº”çš„å›¾ç‰‡ç›®å½•ï¼ˆURL hash: {url_hash}ï¼‰...")
    
    if os.path.exists(IMAGE_DIR):
        found_images = False
        for img_dir in os.listdir(IMAGE_DIR):
            if img_dir.endswith(f"_{url_hash}"):
                img_path = os.path.join(IMAGE_DIR, img_dir)
                try:
                    shutil.rmtree(img_path)
                    logger.info(f"âœ… å·²åˆ é™¤å›¾ç‰‡ç›®å½•: {img_path}")
                    found_images = True
                    deleted = True
                except Exception as e:
                    logger.error(f"âŒ åˆ é™¤å›¾ç‰‡ç›®å½•å¤±è´¥: {img_path} | {e}")
        
        if not found_images:
            logger.info(f"âš ï¸  æœªæ‰¾åˆ°å¯¹åº”çš„å›¾ç‰‡ç›®å½•")
    
    # ä»ä¸‹è½½è®°å½•ä¸­ç§»é™¤
    del articles[url]
    try:
        with open(DOWNLOADED_FILE, "w", encoding="utf-8") as f:
            json.dump({"articles": articles}, f, indent=2, ensure_ascii=False)
        logger.info(f"âœ… å·²ä»ä¸‹è½½è®°å½•ä¸­ç§»é™¤: {url}")
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°ä¸‹è½½è®°å½•å¤±è´¥: {e}")
    
    return deleted


def load_articles():
    """ä»é…ç½®æ–‡ä»¶åŠ è½½æ–‡ç« URLåˆ—è¡¨ï¼ˆæ’é™¤åˆ é™¤æ ‡è®°çš„ï¼‰"""
    if not os.path.exists(CONFIG_FILE):
        logger.error(f"é…ç½®æ–‡ä»¶ {CONFIG_FILE} ä¸å­˜åœ¨")
        return []
    
    try:
        urls = []
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œã€æ³¨é‡Šå’Œåˆ é™¤æ ‡è®°
                if line and not line.startswith("#"):
                    urls.append(line)
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(urls)} ä¸ªæ–‡ç« URL")
        return urls
    except Exception as e:
        logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å‡ºé”™: {e}")
        return []


# ========================


def init_driver():
    logger.info("åˆå§‹åŒ– Chrome æµè§ˆå™¨...")
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--lang=zh-CN")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    )

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)

    logger.info("ChromeDriver åˆå§‹åŒ–å®Œæˆ")
    return driver


def fetch_article_html(driver, url):
    logger.info(f"æ‰“å¼€æ–‡ç« é¡µé¢: {url}")
    driver.get(url)
    time.sleep(4)

    soup = BeautifulSoup(driver.page_source, "html.parser")

    # æå–æ–‡ç« æ ‡é¢˜
    title = None
    title_elem = soup.find("h1", id="js_title")
    if title_elem:
        title = title_elem.get_text(strip=True)
    
    if not title:
        title_tag = soup.find("title")
        if title_tag:
            title = title_tag.get_text(strip=True).replace(" - å¾®ä¿¡å…¬ä¼—å¹³å°", "").strip()
    
    if not title:
        title = "æœªçŸ¥æ ‡é¢˜"
    
    logger.info(f"æ–‡ç« æ ‡é¢˜: {title}")

    content = soup.find("div", id="js_content")
    if content is None:
        logger.warning("æœªæ‰¾åˆ° js_contentï¼Œå°è¯• rich_media_content")
        content = soup.find("div", class_="rich_media_content")

    if content is None:
        logger.error("æ–‡ç« å†…å®¹èŠ‚ç‚¹æœªæ‰¾åˆ°")
        raise Exception("æ— æ³•æ‰¾åˆ°æ–‡ç« å†…å®¹")

    logger.info("æˆåŠŸè§£ææ–‡ç« ä¸»ä½“å†…å®¹")
    return title, content


def download_image(url, filename):
    logger.info(f"ä¸‹è½½å›¾ç‰‡: {url}")
    r = requests.get(url, timeout=15)
    with open(filename, "wb") as f:
        f.write(r.content)


def process_images(html, title, url):
    logger.info("å¤„ç†æ–‡ç« ä¸­çš„å›¾ç‰‡...")
    
    # ç”¨ URL çš„å“ˆå¸Œå€¼æ¥åŒºåˆ†æ–‡ç« ï¼Œç¡®ä¿å”¯ä¸€æ€§ï¼ˆå³ä½¿æ ‡é¢˜é‡å¤ï¼‰
    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
    safe_title = "".join(c if c.isalnum() or c in "._- " else "" for c in title).strip()
    safe_title = safe_title.replace(" ", "_") or "article"
    
    # æ–‡ä»¶å¤¹åæ ¼å¼ï¼šæ ‡é¢˜_å“ˆå¸Œå€¼ï¼ˆå¯è¯»æ€§ + å”¯ä¸€æ€§ï¼‰
    article_image_dir = os.path.join(IMAGE_DIR, f"{safe_title}_{url_hash}")
    os.makedirs(article_image_dir, exist_ok=True)

    soup = BeautifulSoup(str(html), "html.parser")
    imgs = soup.find_all("img")

    logger.info(f"å‘ç° {len(imgs)} å¼ å›¾ç‰‡")

    for i, img in enumerate(imgs):
        src = img.get("data-src") or img.get("src")
        if not src:
            continue

        ext = ".jpg"
        if "png" in src:
            ext = ".png"

        filename = f"img_{i}{ext}"
        filepath = os.path.join(article_image_dir, filename)

        try:
            download_image(src, filepath)
            img["src"] = f"images/{safe_title}_{url_hash}/{filename}"
            logger.info(f"å›¾ç‰‡ä¿å­˜æˆåŠŸ: {filename}")
        except Exception as e:
            logger.error(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {src} | {e}")

    return str(soup)


def get_next_article_number():
    """è·å–ä¸‹ä¸€ä¸ªæ–‡ç« ç¼–å·"""
    markdown_dir = OUTPUT_DIR
    if not os.path.exists(markdown_dir):
        return 1
    
    # æŸ¥æ‰¾ç°æœ‰çš„æ•°å­—æ–‡ä»¶ï¼ˆ00X.md æ ¼å¼ï¼‰
    files = os.listdir(markdown_dir)
    max_num = 0
    for f in files:
        if f[0].isdigit() and f.endswith('.md'):
            try:
                num = int(f.split('.')[0])
                max_num = max(max_num, num)
            except:
                pass
    
    return max_num + 1


def save_markdown(title, html):
    logger.info("è½¬æ¢ä¸º Markdown æ ¼å¼...")
    
    # ç®€å•è½¬æ¢ä¸º markdown
    md_text = md(html)
    
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼ˆä½†ä¿ç•™æ®µè½é—´è·ï¼‰
    lines = md_text.split('\n')
    cleaned_lines = []
    prev_empty = False
    
    for line in lines:
        if line.strip():  # éç©ºè¡Œ
            cleaned_lines.append(line)
            prev_empty = False
        elif not prev_empty:  # åªä¿ç•™ä¸€ä¸ªç©ºè¡Œ
            cleaned_lines.append('')
            prev_empty = True
    
    # ç§»é™¤æœ«å°¾å¤šä½™ç©ºè¡Œ
    while cleaned_lines and not cleaned_lines[-1].strip():
        cleaned_lines.pop()
    
    md_text = '\n'.join(cleaned_lines)
    md_text = f"# {title}\n\n" + md_text

    # ä½¿ç”¨é€’å¢çš„æ•°å­—ä½œä¸ºæ–‡ä»¶åï¼ˆæ›´ç®€æ´çš„ URLï¼‰
    article_num = get_next_article_number()
    filename = f"{article_num:03d}.md"  # 001.md, 002.md ç­‰
    filepath = os.path.join(OUTPUT_DIR, filename)

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(md_text)

    logger.info(f"Markdown æ–‡ä»¶å·²ç”Ÿæˆ: {filepath}")
    return filename, title  # è¿”å›æ–‡ä»¶åå’ŒåŸå§‹æ ‡é¢˜


def sync_mkdocs_nav():
    """åŒæ­¥ mkdocs.yml å¯¼èˆªï¼š
    1. åˆ é™¤ mkdocs.yml ä¸­ä¸å­˜åœ¨çš„æ–‡ä»¶æ¡ç›®
    2. æ·»åŠ  markdown ç›®å½•ä¸­çš„æ–°æ–‡ä»¶
    3. ä¿è¯å¯¼èˆªä¸å®é™…æ–‡ä»¶å®Œå…¨åŒæ­¥
    """
    mkdocs_file = "mkdocs.yml"
    
    try:
        # è·å– markdown ç›®å½•ä¸­æ‰€æœ‰çš„ .md æ–‡ä»¶
        actual_files = set()
        for file in os.listdir(OUTPUT_DIR):
            if file.endswith(".md"):
                actual_files.add(file)
        
        logger.info(f"æ£€æµ‹åˆ°å®é™…æ–‡ä»¶: {actual_files}")
        
        # è¯»å– mkdocs.yml
        with open(mkdocs_file, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f) or {}
        
        if "nav" not in config:
            config["nav"] = []
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦æ˜¾ç¤ºçš„æ–‡ç« ï¼ˆä¸åŒ…æ‹¬ index.mdï¼‰
        articles_to_show = {}  # {filename: title}
        
        # é¦–å…ˆéå†ç°æœ‰å¯¼èˆªï¼Œæ”¶é›†å­˜åœ¨çš„æ–‡ç« ä¿¡æ¯
        for item in config["nav"]:
            if isinstance(item, dict):
                for title, filename in item.items():
                    if filename != "index.md" and filename in actual_files:
                        articles_to_show[filename] = title
        
        # ç„¶åæ‰«æ markdown ç›®å½•ï¼Œæ·»åŠ æœªåœ¨å¯¼èˆªä¸­çš„æ–‡ä»¶
        for filename in actual_files:
            if filename != "index.md" and filename not in articles_to_show:
                title = extract_title_from_file(filename)
                articles_to_show[filename] = title
                logger.info(f"âœ… å‘ç°æ–°æ–‡ç« : '{title}' ({filename})")
        
        # æ„å»ºæ–°çš„å¯¼èˆªï¼šé¦–é¡µ + æŒ‰æ–‡ä»¶åæ’åºçš„æ–‡ç« 
        new_nav = [{"Home": "index.md"}]
        
        for filename in sorted(articles_to_show.keys()):
            title = articles_to_show[filename]
            new_nav.append({title: filename})
        
        # ä¿å­˜æ›´æ–°åçš„é…ç½®
        config["nav"] = new_nav
        with open(mkdocs_file, "w", encoding="utf-8") as f:
            yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
        
        logger.info(f"âœ… mkdocs.yml å·²åŒæ­¥: å…± {len(articles_to_show)} ç¯‡æ–‡ç« ")
            
    except Exception as e:
        logger.error(f"âŒ åŒæ­¥ mkdocs.yml å¤±è´¥: {e}")


def extract_title_from_file(filename):
    """ä» markdown æ–‡ä»¶ä¸­æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œçš„ #ï¼‰"""
    try:
        filepath = os.path.join(OUTPUT_DIR, filename)
        with open(filepath, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line.startswith("#"):
                    # ç§»é™¤ # å’Œç©ºæ ¼ï¼Œè·å–æ ‡é¢˜
                    title = line.lstrip("#").strip()
                    return title if title else filename.replace(".md", "")
        return filename.replace(".md", "")
    except:
        return filename.replace(".md", "")


def update_mkdocs_nav(articles_files):
    """æ›´æ–° mkdocs.yml å¯¼èˆªï¼Œæ·»åŠ æ–°æ–‡ç« 
    articles_files: [(filename, title), ...] å…ƒç»„åˆ—è¡¨
    """
    mkdocs_file = "mkdocs.yml"
    
    try:
        with open(mkdocs_file, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f) or {}
        
        # åˆå§‹åŒ– nav
        if "nav" not in config:
            config["nav"] = []
        
        # è·å–ç°æœ‰çš„å¯¼èˆªé¡¹
        existing_files = set()
        new_nav = []
        
        for item in config["nav"]:
            if isinstance(item, dict):
                for title, path in item.items():
                    existing_files.add(path)
                    new_nav.append(item)
            else:
                new_nav.append(item)
        
        # æ·»åŠ æ–°æ–‡ç« åˆ°å¯¼èˆªï¼ˆä½¿ç”¨åŸå§‹æ ‡é¢˜å’Œæ•°å­—æ–‡ä»¶åï¼‰
        for filename, title in articles_files:
            if filename not in existing_files:
                new_nav.append({title: filename})
                logger.info(f"å°† '{title}' æ·»åŠ åˆ°å¯¼èˆª ({filename})")
        
        config["nav"] = new_nav
        
        # ä¿å­˜æ›´æ–°åçš„é…ç½®
        with open(mkdocs_file, "w", encoding="utf-8") as f:
            yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
        
        logger.info("mkdocs.yml å·²æ›´æ–°")
    except Exception as e:
        logger.error(f"æ›´æ–° mkdocs.yml å¤±è´¥: {e}")


def update_index_page():
    """è‡ªåŠ¨æ›´æ–° index.mdï¼Œç”Ÿæˆæ–‡ç« åˆ—è¡¨"""
    mkdocs_file = "mkdocs.yml"
    index_file = os.path.join(OUTPUT_DIR, "index.md")
    
    try:
        # è¯»å– mkdocs.yml è·å–æ‰€æœ‰æ–‡ç« 
        with open(mkdocs_file, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f) or {}
        
        nav = config.get("nav", [])
        
        # ç”Ÿæˆæ–‡ç« åˆ—è¡¨å†…å®¹
        articles_list = []
        for item in nav:
            if isinstance(item, dict):
                for title, filename in item.items():
                    if filename != "index.md":  # è·³è¿‡é¦–é¡µæœ¬èº«
                        articles_list.append((title, filename))
        
        # æ„å»ºæ–°çš„ index.md å†…å®¹
        index_content = """# WeChat Articles Archive

æ¬¢è¿æ¥åˆ°å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å­˜æ¡£åº“ï¼

è¿™é‡Œæ”¶é›†äº†ç²¾é€‰çš„å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œè½¬æ¢ä¸ºé™æ€ç½‘ç«™æ ¼å¼æ–¹ä¾¿é˜…è¯»ã€‚

## ğŸ“š æ–‡ç« åˆ—è¡¨

"""
        
        if articles_list:
            for i, (title, filename) in enumerate(articles_list, 1):
                # ç”Ÿæˆé“¾æ¥ï¼ˆå»æ‰ .md åç¼€ï¼‰
                link = filename.replace(".md", "")
                index_content += f"{i}. [{title}]({link})\n"
        else:
            index_content += "æš‚æ— æ–‡ç« \n"
        
        # ä¿å­˜æ›´æ–°åçš„ index.md
        with open(index_file, "w", encoding="utf-8") as f:
            f.write(index_content)
        
        logger.info(f"index.md å·²æ›´æ–°ï¼ŒåŒ…å« {len(articles_list)} ç¯‡æ–‡ç« ")
    except Exception as e:
        logger.error(f"æ›´æ–° index.md å¤±è´¥: {e}")


def build_mkdocs():
    """è¿è¡Œ mkdocs build é‡æ–°æ„å»ºç½‘ç«™"""
    try:
        logger.info("å¼€å§‹æ„å»º MkDocs ç½‘ç«™...")
        # åœ¨å½“å‰å·¥ä½œç›®å½•è¿è¡Œï¼Œç¡®ä¿æ‰¾åˆ° mkdocs.yml
        result = subprocess.run(
            ["python", "-m", "mkdocs", "build"],
            cwd=os.getcwd(),
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            logger.info("MkDocs ç½‘ç«™æ„å»ºæˆåŠŸ âœ…")
        else:
            logger.error(f"MkDocs æ„å»ºå¤±è´¥: {result.stderr}")
    except Exception as e:
        logger.error(f"è¿è¡Œ mkdocs build å‡ºé”™: {e}")


def main():
    logger.info("===== ç¨‹åºå¯åŠ¨ =====")
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    has_changes = False  # æ ‡è®°æ˜¯å¦æœ‰æ–‡ç« æ”¹åŠ¨
    article_files_to_update = []  # æ–°æ·»åŠ çš„æ–‡ç« 

    # ============ ç¬¬ä¸€æ­¥ï¼šå¤„ç†åˆ é™¤æ“ä½œ ============
    logger.info("\n--- ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤æ–‡ç«  ---")
    delete_urls = load_delete_urls()
    if delete_urls:
        logger.info(f"å‡†å¤‡åˆ é™¤ {len(delete_urls)} ç¯‡æ–‡ç« ...")
        for url in delete_urls:
            if delete_article(url):
                has_changes = True
                logger.info(f"âœ… æ–‡ç« å·²åˆ é™¤: {url}")
    else:
        logger.info("æ— éœ€åˆ é™¤æ–‡ç« ")

    # ============ ç¬¬äºŒæ­¥ï¼šå¤„ç†ä¸‹è½½æ“ä½œ ============
    logger.info("\n--- ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ–°æ–‡ç« éœ€è¦ä¸‹è½½ ---")
    urls = load_articles()
    if not urls:
        logger.warning("âš ï¸  æ²¡æœ‰æ–‡ç«  URL éœ€è¦å¤„ç†")
    else:
        # åŠ è½½å·²ä¸‹è½½çš„URL
        downloaded_urls = load_downloaded_urls()
        logger.info(f"å·²ä¸‹è½½è®°å½•: {len(downloaded_urls)} ç¯‡æ–‡ç« ")
        
        # è¿‡æ»¤å‡ºæœªä¸‹è½½çš„URL
        urls_to_process = [url for url in urls if url not in downloaded_urls]
        
        if urls_to_process:
            logger.info(f"éœ€è¦å¤„ç†: {len(urls_to_process)} ç¯‡æ–°æ–‡ç« ")
            
            driver = init_driver()
            
            for url in urls_to_process:
                try:
                    title, html = fetch_article_html(driver, url)
                    logger.info(f"å¼€å§‹å¤„ç†æ–‡ç« : {title}")
                    html = process_images(html, title, url)
                    filename, actual_title = save_markdown(title, html)
                    article_files_to_update.append((filename, actual_title))
                    
                    # ä¿å­˜å·²ä¸‹è½½çš„URLå’Œæ–‡ä»¶å
                    save_downloaded_url(url, filename)
                    has_changes = True
                    logger.info(f"âœ… æ–‡ç« å®Œæˆ: {title}")
                except Exception as e:
                    logger.error(f"âŒ æ–‡ç« å¤„ç†å¤±è´¥: {url} | {e}")
            
            driver.quit()
            logger.info("æµè§ˆå™¨å·²å…³é—­")
        else:
            logger.info("æ‰€æœ‰æ–‡ç« éƒ½å·²ä¸‹è½½ï¼Œæ— æ–°æ–‡ç« å¤„ç†")
    # ============ ç¬¬ä¸‰æ­¥ï¼šåŒæ­¥ mkdocs.yml ============
    logger.info("\n--- ç¬¬ä¸‰æ­¥ï¼šåŒæ­¥ mkdocs.ymlï¼ˆç¡®ä¿å¯¼èˆªä¸æ–‡ä»¶ä¸€è‡´ï¼‰ ---")
    sync_mkdocs_nav()
    update_index_page()
    
    # ============ ç¬¬å››æ­¥ï¼šé‡å»ºç½‘ç«™ ============
    logger.info("\n--- ç¬¬å››æ­¥ï¼šé‡æ–°æ„å»ºç½‘ç«™ ---")
    build_mkdocs()
    
    logger.info("\n===== å…¨éƒ¨ä»»åŠ¡å®Œæˆ âœ… =====")


if __name__ == "__main__":
    main()
